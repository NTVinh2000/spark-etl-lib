from delta.tables import *
from pyspark.sql.functions import concat_ws, col, lower, upper, lit, current_timestamp, monotonically_increasing_id
import json


def normalize_col(col):
    return col.strip().strip('_').replace(' ', '_')


def create_table_from_df(src_df, tgt_schema, tgt_tbl, isPrint=False, src_schema=None, src_tbl=None):
    tgt_schema = tgt_schema.lower()
    tgt_tbl = tgt_tbl.upper()
    p_tgt = "abfss://datalake@pocadls2.dfs.core.windows.net/{schema}/{table}".format(schema=tgt_schema, table=tgt_tbl)
    if DeltaTable.isDeltaTable(spark, p_tgt) and spark.catalog.tableExists(tgt_schema + '.' + tgt_tbl):
        raise ('contained in datalake and is already delta table')
    elif DeltaTable.isDeltaTable(spark, p_tgt) and spark.catalog.tableExists(tgt_schema + '.' + tgt_tbl) == False:
        raise Exception('exists in data lake but not in delta table')
    src_df = src_df.limit(1)
    column_string = ''
    if tgt_schema == 'fnd' or tgt_schema == 'dwh' or tgt_schema == 'map':
        src_df = src_df.withColumn('W_SOURCE', lit('')) \
            .withColumn('W_CHECKSUM_SCD1', lit('')) \
            .withColumn('W_CHECKSUM_SCD2', lit('')) \
            .withColumn('W_EFFECT_TO_DT', current_timestamp()) \
            .withColumn('W_EFFECT_FROM_DT', current_timestamp()) \
            .withColumn('W_UPDATE_DT', current_timestamp()) \
            .withColumn('W_INSERT_DT', current_timestamp()) \
            .withColumn('W_INTEGRATION_ID', lit('')) \
            .withColumn('W_PARTITION_COL', lit(' '))
        if tgt_schema == 'dwh' and tgt_tbl.startswith('D_'):
            schema = src_df.dtypes
            column_string = f"{tgt_tbl[2:]}_KEY bigint generated by default as identity,\n"
        for i in schema:
            if i != schema[-1]:
                column_string = column_string + f"{i[0]} {i[1]},\n"
            else:
                column_string = column_string + f"{i[0]} {i[1]}\n"

    create_str = \
        f'''CREATE OR REPLACE TABLE spark_catalog.{tgt_schema}.{tgt_tbl} 
(
    {column_string})
    USING delta
    LOCATION 'abfss://datalake@pocadls2.dfs.core.windows.net/{tgt_schema}/{tgt_tbl}'
    '''
    if isPrint == True:
        print(create_str)
    spark.sql(create_str)


def create_table(src_schema, src_tbl, tgt_schema, tgt_tbl, isPrint=False):
    src_schema = src_schema.lower()
    tgt_schema = tgt_schema.lower()
    src_tbl = src_tbl.upper()
    tgt_tbl = tgt_tbl.upper()
    src_df = spark.sql(f"select * from {src_schema}.{src_tbl}")
    create_table_from_df(src_df, tgt_schema, tgt_tbl, isPrint, src_schema, src_tbl)


#     else:
#         print('Is not delta table and not exist in data lake yet')
# create ddl for datalake without actually load data in
#         schema_df = spark.createDataFrame([], src_df.schema)
#         schema_df.write.format("delta")\
#                 .mode("overwrite")\
#                 .option("overwriteSchema", "true")\
#                 .save(p_tgt)
# #                 .option("truncate", truncate) \

#         tgt_df = DeltaTable.forPath(spark,p_tgt)
#         spark.sql('''DROP TABLE IF EXISTS {schema}.{table}'''.format(schema=tgt_schema, table=tgt_tbl))
#         spark.sql('''
#         CREATE EXTERNAL TABLE {schema}.{table}
#         USING Delta
#         LOCATION "{path}"
#         '''.format(schema=tgt_schema, table=tgt_tbl, path=p_tgt))


def merge_incremental_scd1_df(src_df, tgt_schema, tgt_tbl, truncate):
    tgt_schema = tgt_schema.lower()
    tgt_tbl = tgt_tbl.upper()
    p_tgt = "abfss://datalake@pocadls2.dfs.core.windows.net/{schema}/{table}".format(schema=tgt_schema, table=tgt_tbl)
    #     if tgt_schema == 'fnd':
    #         src_df = src_df.withColumn('W_SOURCE',lit('stg'+'.'+tgt_tbl))\
    #                        .withColumn('W_CHECKSUM_SCD1',lit(''))\
    #                        .withColumn('W_CHECKSUM_SCD2',lit(''))\
    #                        .withColumn('W_EFFECT_TO_DT',current_timestamp())\
    #                        .withColumn('W_EFFECT_FROM_DT',current_timestamp())\
    #                        .withColumn('W_UPDATE_DT',current_timestamp())\
    #                        .withColumn('W_INSERT_DT',current_timestamp())\
    #                        .withColumn('W_PARTITION_COL',lit(''))
    w_list = ['W_SOURCE', 'W_CHECKSUM_SCD1', 'W_CHECKSUM_SCD2', 'W_EFFECT_TO_DT', 'W_EFFECT_FROM_DT', 'W_UPDATE_DT',
              'W_INSERT_DT', 'W_PARTITION_COL']
    for w in w_list:
        if w not in src_df.columns:
            if w == 'W_SOURCE':
                src_df = src_df.withColumn('W_SOURCE', lit('stg' + '.' + tgt_tbl))
            elif w == 'W_CHECKSUM_SCD1' or w == 'W_CHECKSUM_SCD2':
                src_df = src_df.withColumn(w, lit('')) \
                    .withColumn(w, lit('')) \
                    .withColumn(w, lit(''))
            elif w.endswith('_DT'):
                src_df = src_df.withColumn(w, current_timestamp())

    src_df = src_df.withColumn('W_INTEGRATION_ID', upper(col('W_INTEGRATION_ID')))
    if DeltaTable.isDeltaTable(spark, p_tgt) and spark.catalog.tableExists(tgt_schema + '.' + tgt_tbl):
        print('contained in datalake and is delta table')
        tgt_df = DeltaTable.forPath(spark, p_tgt)
    else:
        raise Exception(f'Table {tgt_schema}.{tgt_tbl} not created yet')

    if truncate == True:
        spark.sql('truncate table {schema}.{table}'.format(schema=tgt_schema, table=tgt_tbl))
    #     df = src_df.toDF()

    insert_map = {}
    update_map = {}
    for i in src_df.dtypes:
        if '_KEY' not in i[0].upper():
            insert_map[i[0]] = f'src.{i[0]}'

    for i in src_df.dtypes:
        if '_KEY' not in i[0].upper() and 'W_INSERT_DT' not in i[0].upper():
            update_map[i[0]] = f'src.{i[0]}'

    tgt_df.alias('tgt') \
        .merge(
        src_df.alias('src'),
        'tgt.W_INTEGRATION_ID = src.W_INTEGRATION_ID'
    ) \
        .whenMatchedUpdate(set=update_map) \
        .whenNotMatchedInsert(values=insert_map) \
        .execute()
    #     print(schema_json)
    #     schema_map = json.dumps(schema_json)

    return tgt_df.toDF()


def merge_incremental_scd1(src_schema, src_tbl, tgt_schema, tgt_tbl, w_integration_id, truncate):
    src_schema = src_schema.lower()
    tgt_schema = tgt_schema.lower()
    src_tbl = src_tbl.upper()
    tgt_tbl = tgt_tbl.upper()
    src_df = spark.sql("select * from {schema}.{table}".format(schema=src_schema, table=src_tbl))
    if 'W_INTEGRATION_ID' not in src_df.columns:
        src_df = src_df.withColumn('W_INTEGRATION_ID', concat_ws('~', *src_df[w_integration_id]))
        print('w_integration_id not found in src')
    else:
        print('W_INTEGRATION_ID already exists in src')
    tgt_df = merge_incremental_scd1_df(src_df, tgt_schema, tgt_tbl, truncate)
    return tgt_df

# df = merge_incremental_scd1('stg','NAV_WORKING_CALENDAR_CHANGES','fnd','NAV_WORKING_CALENDAR_CHANGES',['Distributor_Code','Working_Calendar_Code','Date','Line_No'],'true')
# display(df)

# df = merge_incremental_scd1_df(src_df, 'dwh', 'target_table', None, truncate)